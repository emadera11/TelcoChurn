---
title: ""
output: 
    html_document:
        smart: false
---

<h3>What is Customer Churn?</h3>
Simply put, customer churn occurs when customers or subscribers stop doing business with a company or service. Also known as customer attrition, customer churn is a critical metric because it is much less expensive to retain existing customers than it is to acquire new customers – earning business from new customers means working leads all the way through the sales funnel, utilizing your marketing and sales resources throughout the process. Customer retention, on the other hand, is generally more cost-effective as you’ve already earned the trust and loyalty of existing customers.

Customer churn impedes growth, so companies should have a defined method for calculating customer churn in a given period of time. By being aware of and monitoring churn rate, organizations are equipped to determine their customer retention success rates and identify strategies for improvement.


Loading the necessary pacakges for this analysis: 

```{r messages=FALSE}

library(dplyr)
library(psych)
library(tidyr)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(caTools)
library(randomForest)
library(knitr)
library(rmarkdown)
library(kableExtra)
library(gridExtra)
library(corrplot)


```
For this analysis i will predict customer churn using Logistic Regression, Decision Tree and Random Forest.   Data background information: 
THe data contains customer information such as if the customer is senior citizen, which product are they current enroll, 
monthly charges and total charges, amoung other variables that might be important for our prediction.  

The below tables contains information on what is stored in each variable.  This gives us the ability to guage 
on what the content of the data.   

```{r echo=FALSE} 
options(knitr.table.format = "html")
df  = read.csv("D:\\Emman\\Documents\\GitHub\\TelcoChurn\\ChurnAnalysis\\ChurnAnalysis\\datasource\\customerchurn.csv", na.strings = c("", "NA"))

kable(head(df)) %>%
     kable_styling(bootstrap_options = "striped",
                full_width = F) %>%
             scroll_box(width='100%')

```


Below table shows the description of each variable.    before we move further to clean the data and prepare the data for our models 
lets explore the data.   Below I used the summary function to describe the contents of the data.


```{r}
summary(df)                                                                  
```

<h2>Data Visualization and Exploration</h2>

The below Histograms represent the distribution of the MonthlyCHarges, TotalCharges and Tenure.  

<ul>
    <li>MonthlyCharges: The distribution for this data as we can visualize it does not come from a normal distirbution.  We can do a statistical test and hypothize the normality of the data. </li></br>
    <li>TotalCharges: This data is heavily right skewed we can conclude that this data does not come from a normal distribution.</li></br>
    <li>Tenure: This data does not come from a normal distribution. We can perform a statistical analysis to determine the normality of this data. </li>

</ul>

```{r fig.width=20, fig.height=7}

a = ggplot(df, aes(MonthlyCharges)) +
    geom_histogram(color = "black", fill = "red", binwidth = 15, alpha = 0.5) +
    theme_bw()

b = ggplot(df, aes(TotalCharges)) +
    geom_histogram(color = "black", fill = "blue", alpha = 0.5, binwidth = 500) +
    theme_bw()

c = ggplot(df, aes(tenure)) +
    geom_histogram(color = "black", fill = "green", alpha = 0.5, binwidth = 5) +
    theme_bw()

grid.arrange(a, b, c, ncol=3)
```

```{r}
```


The below chart shows the averages Total Monthly Charges by Tenure and Contract.  
As we can see Month to Month and one Year contract have a strong linear corralation .  For the two year Contract
There is a strong linear corralation but is not as strong as the other two contracts.  As the tenure months increase the average monthly charges tends to increase as well. 

The boxplot shows that the average for monthly charges are closed for being equal.  We can use analysis of anavo to test the hypothesis that the mean are equal for the contracts.  
Boxplot are also used to detect outliers from the boxplot below it appears that there aren't any visible outliers.  I will perform a statistical analysis for outlier detection.

```{r fig.width=20, fig.height=7} 
d = df %>% group_by(Contract, tenure) %>%
    summarise(AvgMonthly = mean(MonthlyCharges)) %>%
    ggplot(aes(tenure, AvgMonthly, color = Contract)) + #, group = Contract)) +
    geom_point(size=3) +
    ggtitle("Average Monthly Charges By Tenure") +
    theme_bw()

e = ggplot(df, aes(factor(Contract), MonthlyCharges, fill = factor(Contract))) +
    geom_boxplot() +
    theme_bw() +
    ggtitle("Boxplot for Contract nd Monthly Charges")

grid.arrange(d, e, ncol = 2)

```
<h2>Data Cleansing</h2> 

Data cleansing is a big part in any data analytics project.  The cleansing will allow us to standardize the data and prepare the data for modeling.   
we have noticed that the there are several features that contains the following: 
<ul> 
    <li>No Internet Service </li>
    <li> No Phone Service </li> 
</ul>

These features contains other data such as Yes and No, We will normalize the data to just have a Yes and no.  the below loop will look for those feature and transform them.  

To explain the below code and what the code is doing to clean the data.  the First block of code 
Loops through the dataset columns, then checks which features are factors.  There are 7 columns that contains the following 
values: 
<ul>
    <li>No internet service</li> 
    <li>No phone service</li>
</ul> 

For these columns that contains these data values I am converting them to "NO".   

For the second block of code during the loop I am finding the where there is a 2 factor level recoding these values to 0, 1.  

The Total Charges contains 11 records with null values therefore I used the mean of the columns to impute the missing values.   

Recorded the values for the column Contract.   

For our model I will be scaling the continous variables.    This will improved our model accuracy.

```{r}
for (i in names(df)) {
    if (is.factor(df[, i]) == TRUE) {
        df[, i] = as.factor(ifelse(as.character(df[, i]) == "No internet service" | as.character(df[, i]) == "No phone service", "No", as.character(df[, i])))

    }
    if (is.factor(df[, i]) & nlevels(df[, i]) == 2) {
        if (i == "gender") {
            df[, i] = factor(df[, i], levels = c("Female", "Male"), labels = c(0, 1))
        } else {
            df[, i] = factor(df[, i], levels = c("No", "Yes"), labels = c(0, 1))
        }
    }
}

df$TotalCharges = ifelse(is.na(df$TotalCharges), mean(df$TotalCharges, na.rm = TRUE), df$TotalCharges)

df$InternetService = factor(df$InternetService,
                            levels = c("DSL", "Fiber optic", "No"),
                            labels = c(1, 2, 3))

df$Contract = factor(df$Contract,
                     levels = c("Month-to-month", "One year", "Two year"),
                     labels = c(1, 2, 3))


for (i in colnames(df)) {
    if (is.numeric(df[, i]) & i != "SeniorCitizen") {
        df[, i] = scale(df[, i])
    }
}

summary(df)

final = df %>% select(-customerID, - PaymentMethod)

```

<h2>Model Building</h2> 

Before  we begin the modeling process we need to do split the data into train and test.  

We will use the train data to train our model and the the test data to test our model performance.   I will use 80% of the data 
to train the model and 20% to test the model.  

```{r} 

#split train to test
split = sample.split(final$Churn, SplitRatio = 0.80)
train = subset(final, split == TRUE)
test = subset(final, split == FALSE)

```

Lets see the first 5 rows of the train data.  
```{r}
kable(head(train)) %>% kable_styling(bootstrap_options = "striped",
                full_width = F) %>%
                scroll_box(width = '100%')
```
Lets also see the first 5 rows of the test data. 
```{r}
kable(head(test)) %>% kable_styling(bootstrap_options = "striped",
                full_width = F) %>%
                scroll_box(width = '100%')

```

For this data I will be using to machine learning algorithm, although there are more algorithms to be applied to this data but for right now I will used Logistic Regression and Decision Tree.  We can see how both models will perform and choose the best model based on the accuracy of the  model.   